{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afbc75b",
   "metadata": {},
   "source": [
    "## Install Prefect and necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bacf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prefect==2.0b5\n",
      "  Downloading prefect-2.0b5-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting croniter>=1.0.1\n",
      "  Downloading croniter-1.3.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting aiofiles>=0.7.0\n",
      "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
      "Collecting fastapi>=0.70\n",
      "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting distributed>=2021.08.0\n",
      "  Downloading distributed-2022.5.2-py3-none-any.whl (872 kB)\n",
      "\u001b[K     |████████████████████████████████| 872 kB 64.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-storage>=2.1.0\n",
      "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 71.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rich>=10.0\n",
      "  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 71.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ray[default]>=1.9\n",
      "  Downloading ray-1.13.0-cp39-cp39-manylinux2014_x86_64.whl (54.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 54.3 MB 74.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (6.0)\n",
      "Collecting asgi-lifespan>=1.0\n",
      "  Downloading asgi_lifespan-1.0.1-py3-none-any.whl (10 kB)\n",
      "Collecting coolname>=1.0.4\n",
      "  Downloading coolname-1.1.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting readchar>=3.0.5\n",
      "  Downloading readchar-3.0.5-py3-none-any.whl (7.0 kB)\n",
      "Collecting uvicorn>=0.14.0\n",
      "  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting azure-storage-blob>=12.9.0\n",
      "  Downloading azure_storage_blob-12.12.0-py3-none-any.whl (366 kB)\n",
      "\u001b[K     |████████████████████████████████| 366 kB 73.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click<8.2,>=8.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (8.1.3)\n",
      "Collecting typer>=0.4.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting toml>=0.10.0\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting aiosqlite>=0.17.0\n",
      "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
      "Collecting pydantic>=1.8.2\n",
      "  Downloading pydantic-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 63.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2021.1 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (2022.1)\n",
      "Collecting httpx>=0.22\n",
      "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting anyio>=3.4.0\n",
      "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: alembic>=1.7.5 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (1.7.7)\n",
      "Collecting cryptography>=36.0.1\n",
      "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 51.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy[asyncio]!=1.4.33,>=1.4.20 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (1.4.36)\n",
      "Collecting kubernetes>=17.17.0\n",
      "  Downloading kubernetes-23.6.0-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 67.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: docker>=4.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (5.0.3)\n",
      "Requirement already satisfied: cloudpickle>=1.5 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (2.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.7.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (2022.5.0)\n",
      "Collecting pendulum>=2.1.2\n",
      "  Downloading pendulum-2.1.2-cp39-cp39-manylinux1_x86_64.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 69.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.10.0.1\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: boto3>=1.20.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from prefect==2.0b5) (1.23.10)\n",
      "Collecting asyncpg>=0.23\n",
      "  Downloading asyncpg-0.25.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 68.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-slugify>=5.0\n",
      "  Downloading python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: Mako in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from alembic>=1.7.5->prefect==2.0b5) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from anyio>=3.4.0->prefect==2.0b5) (3.3)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting msrest>=0.6.21\n",
      "  Downloading msrest-0.7.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting azure-core<2.0.0,>=1.23.1\n",
      "  Downloading azure_core-1.24.1-py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 72.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob>=12.9.0->prefect==2.0b5) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.1->azure-storage-blob>=12.9.0->prefect==2.0b5) (2.27.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from boto3>=1.20.0->prefect==2.0b5) (1.0.0)\n",
      "Requirement already satisfied: botocore<1.27.0,>=1.26.10 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from boto3>=1.20.0->prefect==2.0b5) (1.26.10)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from boto3>=1.20.0->prefect==2.0b5) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from botocore<1.27.0,>=1.26.10->boto3>=1.20.0->prefect==2.0b5) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from botocore<1.27.0,>=1.26.10->boto3>=1.20.0->prefect==2.0b5) (1.26.9)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from cryptography>=36.0.1->prefect==2.0b5) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.1->prefect==2.0b5) (2.21)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting locket>=1.0.0\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.11.2-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from distributed>=2021.08.0->prefect==2.0b5) (5.9.1)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from distributed>=2021.08.0->prefect==2.0b5) (6.1)\n",
      "Collecting dask==2022.05.2\n",
      "  Downloading dask-2022.5.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 67.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[K     |████████████████████████████████| 322 kB 55.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from distributed>=2021.08.0->prefect==2.0b5) (21.3)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from distributed>=2021.08.0->prefect==2.0b5) (3.1.2)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from docker>=4.0->prefect==2.0b5) (1.3.2)\n",
      "Collecting starlette==0.19.1\n",
      "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.7.0-py2.py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 72.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.1-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 72.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.1-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<4.0.0dev,>=3.15.0\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 60.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.2-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 73.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (36 kB)\n",
      "Collecting httpcore<0.16.0,>=0.15.0\n",
      "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from httpx>=0.22->prefect==2.0b5) (2022.5.18.1)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting h11<0.13,>=0.11\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=21.0.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from kubernetes>=17.17.0->prefect==2.0b5) (61.2.0)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from packaging>=20.0->distributed>=2021.08.0->prefect==2.0b5) (3.0.9)\n",
      "Collecting pytzdata>=2020.1\n",
      "  Downloading pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\n",
      "\u001b[K     |████████████████████████████████| 489 kB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click<8.2,>=8.0\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting virtualenv\n",
      "  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 59.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from ray[default]>=1.9->prefect==2.0b5) (21.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from ray[default]>=1.9->prefect==2.0b5) (1.22.4)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from ray[default]>=1.9->prefect==2.0b5) (4.5.1)\n",
      "Collecting grpcio<=1.43.0,>=1.28.1\n",
      "  Downloading grpcio-1.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 71.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist\n",
      "  Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 70.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting gpustat>=1.0.0b1\n",
      "  Downloading gpustat-1.0.0b1.tar.gz (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 338 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.12-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp-cors\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting colorful\n",
      "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 72.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp>=3.7\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 9.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting prometheus-client<0.14.0,>=0.7.1\n",
      "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opencensus\n",
      "  Downloading opencensus-0.9.0-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 73.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n",
      "\u001b[K     |████████████████████████████████| 304 kB 53.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 74.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from aiohttp>=3.7->ray[default]>=1.9->prefect==2.0b5) (2.0.12)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting blessed>=1.17.1\n",
      "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]>=1.9->prefect==2.0b5) (0.2.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from requests-oauthlib->kubernetes>=17.17.0->prefect==2.0b5) (3.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from rich>=10.0->prefect==2.0b5) (2.12.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from sqlalchemy[asyncio]!=1.4.33,>=1.4.20->prefect==2.0b5) (1.1.2)\n",
      "Collecting asgiref>=3.4.0\n",
      "  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from jinja2->distributed>=2021.08.0->prefect==2.0b5) (2.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages (from jsonschema->ray[default]>=1.9->prefect==2.0b5) (0.18.1)\n",
      "Collecting opencensus-context>=0.1.2\n",
      "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting platformdirs<3,>=2\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting distlib<1,>=0.3.1\n",
      "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
      "\u001b[K     |████████████████████████████████| 461 kB 71.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gpustat, nvidia-ml-py3\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-1.0.0b1-py3-none-any.whl size=15980 sha256=1090ae3eaf25558ab108e823c51d7b2ff0e4ba51e2a1783f0b44525455b0648b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b5/94/ce/b9757988b38e7e5424d4a0dd3e23683ff4ed16ec25666f9386\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=bcdebabed2d8f5b0868bec88386f03de9ead1f7d5c9f55fc172dd623d8bb0375\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/f6/d8/b0/15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built gpustat nvidia-ml-py3\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, protobuf, multidict, frozenlist, cachetools, yarl, typing-extensions, toolz, sniffio, platformdirs, locket, googleapis-common-protos, google-auth, filelock, distlib, async-timeout, aiosignal, virtualenv, rfc3986, requests-oauthlib, partd, opencensus-context, nvidia-ml-py3, msgpack, isodate, heapdict, h11, grpcio, google-crc32c, google-api-core, click, blessed, azure-core, anyio, aiohttp, zict, text-unidecode, tblib, starlette, sortedcontainers, smart-open, ray, pytzdata, pydantic, py-spy, prometheus-client, opencensus, msrest, httpcore, gpustat, google-resumable-media, google-cloud-core, dask, cryptography, commonmark, colorful, asgiref, aiohttp-cors, uvicorn, typer, toml, rich, readchar, python-slugify, pendulum, kubernetes, httpx, google-cloud-storage, fastapi, distributed, croniter, coolname, azure-storage-blob, asyncpg, asgi-lifespan, aiosqlite, aiofiles, prefect\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.1\n",
      "    Uninstalling protobuf-4.21.1:\n",
      "      Successfully uninstalled protobuf-4.21.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus-client 0.14.1\n",
      "    Uninstalling prometheus-client-0.14.1:\n",
      "      Successfully uninstalled prometheus-client-0.14.1\n",
      "Successfully installed aiofiles-0.8.0 aiohttp-3.8.1 aiohttp-cors-0.7.0 aiosignal-1.2.0 aiosqlite-0.17.0 anyio-3.6.1 asgi-lifespan-1.0.1 asgiref-3.5.2 async-timeout-4.0.2 asyncpg-0.25.0 azure-core-1.24.1 azure-storage-blob-12.12.0 blessed-1.19.1 cachetools-5.2.0 click-8.0.4 colorful-0.5.4 commonmark-0.9.1 coolname-1.1.0 croniter-1.3.5 cryptography-37.0.2 dask-2022.5.2 distlib-0.3.4 distributed-2022.5.2 fastapi-0.78.0 filelock-3.7.1 frozenlist-1.3.0 google-api-core-2.8.1 google-auth-2.7.0 google-cloud-core-2.3.1 google-cloud-storage-2.4.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.2 gpustat-1.0.0b1 grpcio-1.43.0 h11-0.12.0 heapdict-1.0.1 httpcore-0.15.0 httpx-0.23.0 isodate-0.6.1 kubernetes-23.6.0 locket-1.0.0 msgpack-1.0.4 msrest-0.7.0 multidict-6.0.2 nvidia-ml-py3-7.352.0 opencensus-0.9.0 opencensus-context-0.1.2 partd-1.2.0 pendulum-2.1.2 platformdirs-2.5.2 prefect-2.0b5 prometheus-client-0.13.1 protobuf-3.20.1 py-spy-0.3.12 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.9.1 python-slugify-6.1.2 pytzdata-2020.1 ray-1.13.0 readchar-3.0.5 requests-oauthlib-1.3.1 rfc3986-1.5.0 rich-12.4.4 rsa-4.8 smart-open-6.0.0 sniffio-1.2.0 sortedcontainers-2.4.0 starlette-0.19.1 tblib-1.7.0 text-unidecode-1.3 toml-0.10.2 toolz-0.11.2 typer-0.4.1 typing-extensions-4.2.0 uvicorn-0.17.6 virtualenv-20.14.1 yarl-1.7.2 zict-2.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install prefect==2.0b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053a4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89df6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-10 09:52:53--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-01.parquet\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.216.109.59\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.216.109.59|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11886281 (11M) [binary/octet-stream]\n",
      "Saving to: ‘./data/fhv_tripdata_2021-01.parquet’\n",
      "\n",
      "fhv_tripdata_2021-0 100%[===================>]  11.33M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-06-10 09:52:53 (80.7 MB/s) - ‘./data/fhv_tripdata_2021-01.parquet’ saved [11886281/11886281]\n",
      "\n",
      "--2022-06-10 09:52:53--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-02.parquet\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 54.231.172.65\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|54.231.172.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10645466 (10M) [binary/octet-stream]\n",
      "Saving to: ‘./data/fhv_tripdata_2021-02.parquet’\n",
      "\n",
      "fhv_tripdata_2021-0 100%[===================>]  10.15M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-06-10 09:52:53 (103 MB/s) - ‘./data/fhv_tripdata_2021-02.parquet’ saved [10645466/10645466]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-01.parquet -P ./data\n",
    "! wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-02.parquet -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee4f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  fhv_tripdata_2021-01.parquet  fhv_tripdata_2021-02.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! ls -a ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3760adb",
   "metadata": {},
   "source": [
    "## Q1. Converting the script to a Prefect flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2af2a",
   "metadata": {},
   "source": [
    "#### Check homework.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058680ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean duration of training is 16.2472533682457\n",
      "The mean duration of validation is 16.859265811074096\n",
      "The shape of X_train is (1109826, 525)\n",
      "The DictVectorizer has 525 features\n",
      "The MSE of training is: 10.528519403243587\n",
      "The MSE of validation is: 11.014288317703189\n"
     ]
    }
   ],
   "source": [
    "! python homework.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb895367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing homework_prefect_q1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework_prefect_q1.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect import get_run_logger\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "\n",
    "\n",
    "@task\n",
    "def read_data(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def prepare_features(df, categorical, train=True):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    mean_duration = df.duration.mean()\n",
    "    if train:\n",
    "        logger.info(f\"The mean duration of training is {mean_duration}\")\n",
    "    else:\n",
    "        logger.info(f\"The mean duration of validation is {mean_duration}\")\n",
    "    \n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def train_model(df, categorical):\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    train_dicts = df[categorical].to_dict(orient='records')\n",
    "    dv = DictVectorizer()\n",
    "    X_train = dv.fit_transform(train_dicts) \n",
    "    y_train = df.duration.values\n",
    "\n",
    "    logger.info(f\"The shape of X_train is {X_train.shape}\")\n",
    "    logger.info(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of training is: {mse}\")\n",
    "    return lr, dv\n",
    "\n",
    "@task\n",
    "def run_model(df, categorical, dv, lr):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    val_dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts) \n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_val = df.duration.values\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of validation is: {mse}\")\n",
    "    return\n",
    "\n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(train_path: str = './data/fhv_tripdata_2021-01.parquet', \n",
    "           val_path: str = './data/fhv_tripdata_2021-02.parquet'):\n",
    "\n",
    "    categorical = ['PUlocationID', 'DOlocationID']\n",
    "\n",
    "    df_train = read_data(train_path)\n",
    "    df_train_processed = prepare_features(df_train, categorical)\n",
    "\n",
    "    df_val = read_data(val_path)\n",
    "    df_val_processed = prepare_features(df_val, categorical, False)\n",
    "\n",
    "    # train the model\n",
    "    lr, dv = train_model(df_train_processed, categorical).result()\n",
    "    run_model(df_val_processed, categorical, dv, lr)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "157cfe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:02:13.743 | INFO    | prefect.engine - Created flow run 'nocturnal-horse' for flow 'main'\n",
      "13:02:13.744 | INFO    | Flow run 'nocturnal-horse' - Using task runner 'SequentialTaskRunner'\n",
      "13:02:13.759 | WARNING | Flow run 'nocturnal-horse' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "13:02:13.828 | INFO    | Flow run 'nocturnal-horse' - Created task run 'read_data-4c7f9de4-0' for task 'read_data'\n",
      "13:02:18.997 | INFO    | Task run 'read_data-4c7f9de4-0' - Finished in state Completed()\n",
      "13:02:19.040 | INFO    | Flow run 'nocturnal-horse' - Created task run 'prepare_features-4ee39d9f-0' for task 'prepare_features'\n",
      "13:02:19.258 | INFO    | Task run 'prepare_features-4ee39d9f-0' - The mean duration of training is 16.2472533682457\n",
      "13:02:26.484 | INFO    | Task run 'prepare_features-4ee39d9f-0' - Finished in state Completed()\n",
      "13:02:26.524 | INFO    | Flow run 'nocturnal-horse' - Created task run 'read_data-4c7f9de4-1' for task 'read_data'\n",
      "13:02:31.115 | INFO    | Task run 'read_data-4c7f9de4-1' - Finished in state Completed()\n",
      "13:02:31.153 | INFO    | Flow run 'nocturnal-horse' - Created task run 'prepare_features-4ee39d9f-1' for task 'prepare_features'\n",
      "13:02:31.345 | INFO    | Task run 'prepare_features-4ee39d9f-1' - The mean duration of validation is 16.859265811074096\n",
      "13:02:37.692 | INFO    | Task run 'prepare_features-4ee39d9f-1' - Finished in state Completed()\n",
      "13:02:37.732 | INFO    | Flow run 'nocturnal-horse' - Created task run 'train_model-7c866860-0' for task 'train_model'\n",
      "13:02:42.842 | INFO    | Task run 'train_model-7c866860-0' - The shape of X_train is (1109826, 525)\n",
      "13:02:42.842 | INFO    | Task run 'train_model-7c866860-0' - The DictVectorizer has 525 features\n",
      "13:02:47.932 | INFO    | Task run 'train_model-7c866860-0' - The MSE of training is: 10.528519403243587\n",
      "13:02:48.079 | INFO    | Task run 'train_model-7c866860-0' - Finished in state Completed()\n",
      "13:02:48.109 | INFO    | Flow run 'nocturnal-horse' - Created task run 'run_model-6559300c-0' for task 'run_model'\n",
      "13:02:52.529 | INFO    | Task run 'run_model-6559300c-0' - The MSE of validation is: 11.014288317703189\n",
      "13:02:52.646 | INFO    | Task run 'run_model-6559300c-0' - Finished in state Completed()\n",
      "13:02:59.306 | INFO    | Flow run 'nocturnal-horse' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "! python homework_prefect_q1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443a01f",
   "metadata": {},
   "source": [
    "## Q2. Parameterizing the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b87b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework_prefect_q2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework_prefect_q2.py\n",
    "\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect import get_run_logger\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "\n",
    "\n",
    "DATA_URL = 'https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{year}-{month}.parquet'\n",
    "DATA_DIR = './data'\n",
    "LAST_DATA_DATE = date(2022, 3, 1)\n",
    "\n",
    "\n",
    "@task\n",
    "def read_data(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def prepare_features(df, categorical, train=True):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    mean_duration = df.duration.mean()\n",
    "    if train:\n",
    "        logger.info(f\"The mean duration of training is {mean_duration}\")\n",
    "    else:\n",
    "        logger.info(f\"The mean duration of validation is {mean_duration}\")\n",
    "    \n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def train_model(df, categorical):\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    train_dicts = df[categorical].to_dict(orient='records')\n",
    "    dv = DictVectorizer()\n",
    "    X_train = dv.fit_transform(train_dicts) \n",
    "    y_train = df.duration.values\n",
    "\n",
    "    logger.info(f\"The shape of X_train is {X_train.shape}\")\n",
    "    logger.info(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of training is: {mse}\")\n",
    "    return lr, dv\n",
    "\n",
    "\n",
    "@task\n",
    "def run_model(df, categorical, dv, lr):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    val_dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts) \n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_val = df.duration.values\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of validation is: {mse}\")\n",
    "    return\n",
    "\n",
    "\n",
    "@task\n",
    "def get_paths(date: (None, str)):\n",
    "    date = datetime.now() if not date else datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    train_date = date - dateutil.relativedelta.relativedelta(months=2)\n",
    "    val_date = date - dateutil.relativedelta.relativedelta(months=1)\n",
    "    \n",
    "    train_path = download_source_data(year=train_date.year, month=train_date.month)\n",
    "    val_path = download_source_data(year=val_date.year, month=val_date.month)\n",
    "    \n",
    "    return train_path, val_path\n",
    "    \n",
    "\n",
    "def download_source_data(year: int, month: int):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    if LAST_DATA_DATE.year < year or (LAST_DATA_DATE.year == year and LAST_DATA_DATE.month < month):\n",
    "        raise ValueError(f\"No taxi data for such date: {year}-{month}\")\n",
    "    \n",
    "    url = DATA_URL.format(year=year, month=str(month).zfill(2))\n",
    "    path = os.path.join(DATA_DIR, os.path.basename(url))\n",
    "    logger.info(f\"Downloading file: {url}\")\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(date=None):\n",
    "    train_path, val_path = get_paths(date).result()\n",
    "    \n",
    "    categorical = ['PUlocationID', 'DOlocationID']\n",
    "\n",
    "    df_train = read_data(train_path)\n",
    "    df_train_processed = prepare_features(df_train, categorical)\n",
    "\n",
    "    df_val = read_data(val_path)\n",
    "    df_val_processed = prepare_features(df_val, categorical, False)\n",
    "\n",
    "    # train the model\n",
    "    lr, dv = train_model(df_train_processed, categorical).result()\n",
    "    run_model(df_val_processed, categorical, dv, lr)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(date=\"2021-08-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa53f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:58:49.695 | INFO    | prefect.engine - Created flow run 'wooden-coucal' for flow 'main'\n",
      "16:58:49.696 | INFO    | Flow run 'wooden-coucal' - Using task runner 'SequentialTaskRunner'\n",
      "16:58:49.843 | INFO    | Flow run 'wooden-coucal' - Created task run 'get_paths-6e696e34-0' for task 'get_paths'\n",
      "16:58:49.882 | INFO    | Task run 'get_paths-6e696e34-0' - Downloading file: https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-06.parquet\n",
      "16:58:50.166 | INFO    | Task run 'get_paths-6e696e34-0' - Downloading file: https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-07.parquet\n",
      "16:58:50.525 | INFO    | Task run 'get_paths-6e696e34-0' - Finished in state Completed()\n",
      "16:58:50.557 | INFO    | Flow run 'wooden-coucal' - Created task run 'read_data-4c7f9de4-0' for task 'read_data'\n",
      "16:58:56.454 | INFO    | Task run 'read_data-4c7f9de4-0' - Finished in state Completed()\n",
      "16:58:56.494 | INFO    | Flow run 'wooden-coucal' - Created task run 'prepare_features-4ee39d9f-0' for task 'prepare_features'\n",
      "16:58:56.736 | INFO    | Task run 'prepare_features-4ee39d9f-0' - The mean duration of training is 18.230538791569113\n",
      "16:59:04.483 | INFO    | Task run 'prepare_features-4ee39d9f-0' - Finished in state Completed()\n",
      "16:59:04.531 | INFO    | Flow run 'wooden-coucal' - Created task run 'read_data-4c7f9de4-1' for task 'read_data'\n",
      "16:59:10.093 | INFO    | Task run 'read_data-4c7f9de4-1' - Finished in state Completed()\n",
      "16:59:10.131 | INFO    | Flow run 'wooden-coucal' - Created task run 'prepare_features-4ee39d9f-1' for task 'prepare_features'\n",
      "16:59:10.357 | INFO    | Task run 'prepare_features-4ee39d9f-1' - The mean duration of validation is 17.91113046137945\n",
      "16:59:17.535 | INFO    | Task run 'prepare_features-4ee39d9f-1' - Finished in state Completed()\n",
      "16:59:17.578 | INFO    | Flow run 'wooden-coucal' - Created task run 'train_model-7c866860-0' for task 'train_model'\n",
      "16:59:23.418 | INFO    | Task run 'train_model-7c866860-0' - The shape of X_train is (1222031, 525)\n",
      "16:59:23.418 | INFO    | Task run 'train_model-7c866860-0' - The DictVectorizer has 525 features\n",
      "16:59:28.297 | INFO    | Task run 'train_model-7c866860-0' - The MSE of training is: 11.789353639555292\n",
      "16:59:28.449 | INFO    | Task run 'train_model-7c866860-0' - Finished in state Completed()\n",
      "16:59:28.479 | INFO    | Flow run 'wooden-coucal' - Created task run 'run_model-6559300c-0' for task 'run_model'\n",
      "16:59:33.734 | INFO    | Task run 'run_model-6559300c-0' - The MSE of validation is: 11.637028964002573\n",
      "16:59:33.863 | INFO    | Task run 'run_model-6559300c-0' - Finished in state Completed()\n",
      "16:59:47.141 | INFO    | Flow run 'wooden-coucal' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "! python homework_prefect_q2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b480de",
   "metadata": {},
   "source": [
    "## Q3. Saving the model and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "547bd584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting homework_prefect_q3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile homework_prefect_q3.py\n",
    "\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect import get_run_logger\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "\n",
    "\n",
    "DATA_URL = 'https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{year}-{month}.parquet'\n",
    "DATA_DIR = './data'\n",
    "LAST_DATA_DATE = date(2022, 3, 1)\n",
    "\n",
    "\n",
    "@task\n",
    "def read_data(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def prepare_features(df, categorical, train=True):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    mean_duration = df.duration.mean()\n",
    "    if train:\n",
    "        logger.info(f\"The mean duration of training is {mean_duration}\")\n",
    "    else:\n",
    "        logger.info(f\"The mean duration of validation is {mean_duration}\")\n",
    "    \n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def train_model(df, categorical, date: str):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    train_dicts = df[categorical].to_dict(orient='records')\n",
    "    dv = DictVectorizer()\n",
    "    X_train = dv.fit_transform(train_dicts) \n",
    "    y_train = df.duration.values\n",
    "\n",
    "    logger.info(f\"The shape of X_train is {X_train.shape}\")\n",
    "    logger.info(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    \n",
    "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of training is: {mse}\")\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    return lr, dv\n",
    "\n",
    "\n",
    "@task\n",
    "def run_model(df, categorical, dv, lr):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    val_dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts) \n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_val = df.duration.values\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of validation is: {mse}\")\n",
    "    return\n",
    "\n",
    "\n",
    "@task\n",
    "def get_paths(date: (None, str)):\n",
    "    date = datetime.now() if not date else datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    train_date = date - dateutil.relativedelta.relativedelta(months=2)\n",
    "    val_date = date - dateutil.relativedelta.relativedelta(months=1)\n",
    "    \n",
    "    train_path = download_source_data(year=train_date.year, month=train_date.month)\n",
    "    val_path = download_source_data(year=val_date.year, month=val_date.month)\n",
    "    \n",
    "    return train_path, val_path\n",
    "    \n",
    "\n",
    "def download_source_data(year: int, month: int):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    if LAST_DATA_DATE.year < year or (LAST_DATA_DATE.year == year and LAST_DATA_DATE.month < month):\n",
    "        raise ValueError(f\"No taxi data for such date: {year}-{month}\")\n",
    "    \n",
    "    url = DATA_URL.format(year=year, month=str(month).zfill(2))\n",
    "    path = os.path.join(DATA_DIR, os.path.basename(url))\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        logger.info(f\"Using existing file: {path}\")\n",
    "        return path\n",
    "    \n",
    "    logger.info(f\"Downloading file: {url}\")\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(date=None):\n",
    "    mlflow.set_tracking_uri(\"sqlite:////home/ubuntu/mlops-zoomcamp/mlflow/mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment-w3\")\n",
    "    \n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "    \n",
    "        train_path, val_path = get_paths(date).result()\n",
    "        \n",
    "        categorical = ['PUlocationID', 'DOlocationID']\n",
    "    \n",
    "        df_train = read_data(train_path)\n",
    "        df_train_processed = prepare_features(df_train, categorical)\n",
    "    \n",
    "        df_val = read_data(val_path)\n",
    "        df_val_processed = prepare_features(df_val, categorical, False)\n",
    "    \n",
    "        # train the model\n",
    "        \n",
    "        lr, dv = train_model(df_train_processed, categorical, date).result()\n",
    "        \n",
    "        with open(f\"./models/model-{date}.bin\", \"wb\") as f_out:\n",
    "            pickle.dump(lr, f_out)\n",
    "            \n",
    "        with open(f\"./preprocessors/preprocessor-{date}.p\", \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        mlflow.sklearn.log_model(lr, artifact_path=\"models\")\n",
    "    \n",
    "        mlflow.log_artifact(f\"preprocessors/preprocessor-{date}.p\", artifact_path=\"preprocessors\")\n",
    "        \n",
    "        run_model(df_val_processed, categorical, dv, lr)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(date=\"2021-08-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7c4de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:59:21.233 | INFO    | prefect.engine - Created flow run 'poetic-albatross' for flow 'main'\n",
      "17:59:21.233 | INFO    | Flow run 'poetic-albatross' - Using task runner 'SequentialTaskRunner'\n",
      "17:59:22.235 | INFO    | Flow run 'poetic-albatross' - Created task run 'get_paths-6e696e34-0' for task 'get_paths'\n",
      "17:59:22.276 | INFO    | Task run 'get_paths-6e696e34-0' - Using existing file: ./data/fhv_tripdata_2021-06.parquet\n",
      "17:59:22.276 | INFO    | Task run 'get_paths-6e696e34-0' - Using existing file: ./data/fhv_tripdata_2021-07.parquet\n",
      "17:59:22.304 | INFO    | Task run 'get_paths-6e696e34-0' - Finished in state Completed()\n",
      "17:59:22.337 | INFO    | Flow run 'poetic-albatross' - Created task run 'read_data-4c7f9de4-0' for task 'read_data'\n",
      "17:59:28.267 | INFO    | Task run 'read_data-4c7f9de4-0' - Finished in state Completed()\n",
      "17:59:28.308 | INFO    | Flow run 'poetic-albatross' - Created task run 'prepare_features-4ee39d9f-0' for task 'prepare_features'\n",
      "17:59:28.553 | INFO    | Task run 'prepare_features-4ee39d9f-0' - The mean duration of training is 18.230538791569113\n",
      "17:59:36.248 | INFO    | Task run 'prepare_features-4ee39d9f-0' - Finished in state Completed()\n",
      "17:59:36.291 | INFO    | Flow run 'poetic-albatross' - Created task run 'read_data-4c7f9de4-1' for task 'read_data'\n",
      "17:59:41.837 | INFO    | Task run 'read_data-4c7f9de4-1' - Finished in state Completed()\n",
      "17:59:41.878 | INFO    | Flow run 'poetic-albatross' - Created task run 'prepare_features-4ee39d9f-1' for task 'prepare_features'\n",
      "17:59:42.093 | INFO    | Task run 'prepare_features-4ee39d9f-1' - The mean duration of validation is 17.91113046137945\n",
      "17:59:49.301 | INFO    | Task run 'prepare_features-4ee39d9f-1' - Finished in state Completed()\n",
      "17:59:49.343 | INFO    | Flow run 'poetic-albatross' - Created task run 'train_model-7c866860-0' for task 'train_model'\n",
      "17:59:54.983 | INFO    | Task run 'train_model-7c866860-0' - The shape of X_train is (1222031, 525)\n",
      "17:59:54.983 | INFO    | Task run 'train_model-7c866860-0' - The DictVectorizer has 525 features\n",
      "2022/06/12 18:00:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/ubuntu/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\"\n",
      "18:00:02.340 | INFO    | Task run 'train_model-7c866860-0' - The MSE of training is: 11.789353639555292\n",
      "18:00:02.623 | INFO    | Task run 'train_model-7c866860-0' - Finished in state Completed()\n",
      "18:00:04.404 | INFO    | Flow run 'poetic-albatross' - Created task run 'run_model-6559300c-0' for task 'run_model'\n",
      "18:00:09.772 | INFO    | Task run 'run_model-6559300c-0' - The MSE of validation is: 11.637028964002573\n",
      "18:00:09.913 | INFO    | Task run 'run_model-6559300c-0' - Finished in state Completed()\n",
      "18:00:22.813 | INFO    | Flow run 'poetic-albatross' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "! python homework_prefect_q3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c385e6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 13K Jun 12 17:35 preprocessor-2021-08-15.p\r\n"
     ]
    }
   ],
   "source": [
    "! ls -hl ./preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9524a",
   "metadata": {},
   "source": [
    "## Q4. Creating a deployment with a CronSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e25df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp homework_prefect_q3.py hw_flow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d5ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_prefect_deployment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_prefect_deployment.py\n",
    "\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect import get_run_logger\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "\n",
    "\n",
    "DATA_URL = 'https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{year}-{month}.parquet'\n",
    "DATA_DIR = './data'\n",
    "LAST_DATA_DATE = date(2022, 3, 1)\n",
    "\n",
    "\n",
    "@task\n",
    "def read_data(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def prepare_features(df, categorical, train=True):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    mean_duration = df.duration.mean()\n",
    "    if train:\n",
    "        logger.info(f\"The mean duration of training is {mean_duration}\")\n",
    "    else:\n",
    "        logger.info(f\"The mean duration of validation is {mean_duration}\")\n",
    "    \n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def train_model(df, categorical, date: str):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    train_dicts = df[categorical].to_dict(orient='records')\n",
    "    dv = DictVectorizer()\n",
    "    X_train = dv.fit_transform(train_dicts) \n",
    "    y_train = df.duration.values\n",
    "\n",
    "    logger.info(f\"The shape of X_train is {X_train.shape}\")\n",
    "    logger.info(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    \n",
    "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of training is: {mse}\")\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    return lr, dv\n",
    "\n",
    "\n",
    "@task\n",
    "def run_model(df, categorical, dv, lr):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    val_dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts) \n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_val = df.duration.values\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of validation is: {mse}\")\n",
    "    return\n",
    "\n",
    "\n",
    "@task\n",
    "def get_paths(date: (None, str)):\n",
    "    date = datetime.now() if not date else datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    train_date = date - dateutil.relativedelta.relativedelta(months=2)\n",
    "    val_date = date - dateutil.relativedelta.relativedelta(months=1)\n",
    "    \n",
    "    train_path = download_source_data(year=train_date.year, month=train_date.month)\n",
    "    val_path = download_source_data(year=val_date.year, month=val_date.month)\n",
    "    \n",
    "    return train_path, val_path\n",
    "    \n",
    "\n",
    "def download_source_data(year: int, month: int):\n",
    "    logger = get_run_logger()\n",
    "    \n",
    "    if LAST_DATA_DATE.year < year or (LAST_DATA_DATE.year == year and LAST_DATA_DATE.month < month):\n",
    "        raise ValueError(f\"No taxi data for such date: {year}-{month}\")\n",
    "    \n",
    "    url = DATA_URL.format(year=year, month=str(month).zfill(2))\n",
    "    path = os.path.join(DATA_DIR, os.path.basename(url))\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        logger.info(f\"Using existing file: {path}\")\n",
    "        return path\n",
    "    \n",
    "    logger.info(f\"Downloading file: {url}\")\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(date=None):\n",
    "    mlflow.set_tracking_uri(\"sqlite:////home/ubuntu/mlops-zoomcamp/mlflow/mlflow.db\")\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment-w3\")\n",
    "    \n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "    \n",
    "        train_path, val_path = get_paths(date).result()\n",
    "        \n",
    "        categorical = ['PUlocationID', 'DOlocationID']\n",
    "    \n",
    "        df_train = read_data(train_path)\n",
    "        df_train_processed = prepare_features(df_train, categorical)\n",
    "    \n",
    "        df_val = read_data(val_path)\n",
    "        df_val_processed = prepare_features(df_val, categorical, False)\n",
    "    \n",
    "        # train the model\n",
    "        \n",
    "        lr, dv = train_model(df_train_processed, categorical, date).result()\n",
    "        \n",
    "        with open(f\"./models/model-{date}.bin\", \"wb\") as f_out:\n",
    "            pickle.dump(lr, f_out)\n",
    "            \n",
    "        with open(f\"./preprocessors/preprocessor-{date}.p\", \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        mlflow.sklearn.log_model(lr, artifact_path=\"models\")\n",
    "    \n",
    "        mlflow.log_artifact(f\"preprocessors/preprocessor-{date}.p\", artifact_path=\"preprocessors\")\n",
    "        \n",
    "        run_model(df_val_processed, categorical, dv, lr)\n",
    "\n",
    "        \n",
    "from prefect.orion.schemas.schedules import CronSchedule\n",
    "from prefect.flow_runners import SubprocessFlowRunner\n",
    "from prefect.deployments import DeploymentSpec\n",
    "\n",
    "\n",
    "DeploymentSpec(\n",
    "    flow=main,\n",
    "    name=\"model_training\",\n",
    "    schedule=CronSchedule(\n",
    "        cron=\"0 9 15 * *\",\n",
    "        timezone=\"Europe/Belgrade\"\n",
    "    ),\n",
    "    tags=[\"ml\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b38326ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading deployment specifications from python script at \n",
      "\u001b[32m'hw_prefect_deployment.py'\u001b[0m...\n",
      "Creating deployment \u001b[1;34m'model_training'\u001b[0m for flow \u001b[34m'main'\u001b[0m...\n",
      "Deploying flow script from \n",
      "\u001b[32m'/home/ubuntu/mlops-zoomcamp/03-orchestration/hw_prefect_deployment.py'\u001b[0m using S3\n",
      "Storage...\n",
      "Created deployment \u001b[34m'main/\u001b[0m\u001b[1;34mmodel_training'\u001b[0m.\n",
      "View your new deployment with: \n",
      "\n",
      "    prefect deployment inspect \u001b[34m'main/\u001b[0m\u001b[1;34mmodel_training'\u001b[0m\n",
      "\u001b[32mCreated 1 deployments!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! prefect deployment create hw_prefect_deployment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61631d",
   "metadata": {},
   "source": [
    "## Q5. Viewing the Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b25b6c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                                   Flow Runs                                    \u001b[0m\r\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓\r\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                               ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mState\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWhen       \u001b[0m\u001b[1m \u001b[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m68f79162-ffac-4f60-a0db-f0153db6…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mfervent-dog   \u001b[0m\u001b[32m \u001b[0m│ SCHE… │\u001b[1m \u001b[0m\u001b[1min 3 months\u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36me22aa966-9b9c-406a-8d42-51b12416…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mamusing-corgi \u001b[0m\u001b[32m \u001b[0m│ SCHE… │\u001b[1m \u001b[0m\u001b[1min 2 months\u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m343a49ba-7e52-4ef0-9f2c-b6560cf1…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32moffbeat-seal  \u001b[0m\u001b[32m \u001b[0m│ SCHE… │\u001b[1m \u001b[0m\u001b[1min 1 month \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m3d6578b9-e136-49f0-8d55-591328ba…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mdevious-crane \u001b[0m\u001b[32m \u001b[0m│ SCHE… │\u001b[1m \u001b[0m\u001b[1min 2 days  \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36md1415a7f-c1ec-49e0-ab4e-c87ee8bd…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mpoetic-albatr…\u001b[0m\u001b[32m \u001b[0m│ COMP… │\u001b[1m \u001b[0m\u001b[1m51 minutes…\u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36md10b697d-142d-4f38-b633-4d1532e7…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32malmond-mantis \u001b[0m\u001b[32m \u001b[0m│ COMP… │\u001b[1m \u001b[0m\u001b[1m57 minutes…\u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m9fd4f8e2-13dd-44a8-9b17-829e3d1a…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mebony-eagle   \u001b[0m\u001b[32m \u001b[0m│ COMP… │\u001b[1m \u001b[0m\u001b[1m1 hour ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m8f74d408-33a7-4c61-a741-749918d5…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mskilled-marten\u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m1 hour ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36mb5c09129-802e-4ec5-b88f-d1a9ba72…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mrational-hoat…\u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m1 hour ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36mf5b8515e-c0ce-4915-b16b-74e7cb75…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mtidy-centipede\u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m1 hour ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m98b7fdfd-5572-4136-8633-8f4254e2…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhumongous-gor…\u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m2 days ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m83bca15e-8f77-470e-ac6c-002ccf20…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mlovely-dinosa…\u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m2 days ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m385da29e-897b-42b2-b89c-6380de6f…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32meminent-guan  \u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m2 days ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36mfb491726-fe01-4629-9199-e52d8f56…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mvictorious-tu…\u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m2 days ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "│\u001b[36m \u001b[0m\u001b[36m738a1548-6d8a-4052-8f67-21e53b27…\u001b[0m\u001b[36m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m…\u001b[0m\u001b[34m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mwoodoo-cuckoo \u001b[0m\u001b[32m \u001b[0m│ FAIL… │\u001b[1m \u001b[0m\u001b[1m2 days ago \u001b[0m\u001b[1m \u001b[0m│\r\n",
      "└───────────────────────────────────┴───┴────────────────┴───────┴─────────────┘\r\n"
     ]
    }
   ],
   "source": [
    "! prefect flow-run ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13104152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer (option \"3\" was used)\n",
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d390068",
   "metadata": {},
   "source": [
    "## Q6. Creating a work-queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "449a15a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\r\n",
      "┃\u001b[1m \u001b[0m\u001b[1mScheduled Sta…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRun ID                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNa…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDeployment ID            \u001b[0m\u001b[1m \u001b[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\r\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-09-15 07…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m68f79162-ffac-4f60-a0db-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mfe…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m6afdb62e-7900-4a87-ba6f-…\u001b[0m\u001b[34m \u001b[0m│\r\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-08-15 07…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36me22aa966-9b9c-406a-8d42-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mam…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m6afdb62e-7900-4a87-ba6f-…\u001b[0m\u001b[34m \u001b[0m│\r\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-07-15 07…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m343a49ba-7e52-4ef0-9f2c-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mof…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m6afdb62e-7900-4a87-ba6f-…\u001b[0m\u001b[34m \u001b[0m│\r\n",
      "│\u001b[33m \u001b[0m\u001b[33m2022-06-15 07…\u001b[0m\u001b[33m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m3d6578b9-e136-49f0-8d55-…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mde…\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m6afdb62e-7900-4a87-ba6f-…\u001b[0m\u001b[34m \u001b[0m│\r\n",
      "└────────────────┴───────────────────────────┴─────┴───────────────────────────┘\r\n",
      "\u001b[31m                            (**) denotes a late run                             \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! prefect work-queue preview dd921f99-100b-4052-956a-88b48742213d --hours 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b51be164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                            Work Queues                            \u001b[0m\r\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\r\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                                  ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mName\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcurrency Limit\u001b[0m\u001b[1m \u001b[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\r\n",
      "│\u001b[36m \u001b[0m\u001b[36mdd921f99-100b-4052-956a-88b48742213d\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mtaxi\u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34mNone\u001b[0m\u001b[34m             \u001b[0m\u001b[34m \u001b[0m│\r\n",
      "└──────────────────────────────────────┴──────┴───────────────────┘\r\n",
      "\u001b[31m                    (**) denotes a paused queue                    \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! prefect work-queue ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
